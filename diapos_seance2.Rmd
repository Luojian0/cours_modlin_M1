---
title: "Modèle linéaire"
subtitle: "Séance 2 - Régression linéaire simple et multiple"
author: "Florent Chuffart & Magali Richard"
date: "10 Janvier 2019"
output: slidy_presentation
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, width = 75)
knitr::opts_chunk$set(echo = TRUE, fig.align='center', dev='png', dpi = 95, out.width = "100%")
```

---

## Evaluation

 - individuelle sur un jeu de données aprés 4 séances de cours 
 - data challenge à la fin des séances

## Pré-requis
 
 - R https://cran.r-project.org 
 - RStudio https://www.rstudio.com

## Cours 

- https://github.com/magrichard/cours_modlin_M1

---

# Organisation prévisionnelle

Séance 1 - 3h (10/01) Régression linéaire simple

Séance 2 - 3h (24/01) Régression linéaire multiple

Séance 3 - 3h (31/01) Anova 1 facteur / partie A

Séance 4 - 3h (14/02) Anova 1 facteur / partie B

Séance 5 - 3h (21/02) Anova 2 facteurs

Séance 6 - 3h (07/03) Anova multiple

Séance 7 - 3h (11/03) Anova facteurs emboités

Séance 8 - 3h (14/03) Ancova

---

# Plan séance 2
## Régression linéaire multiple


I) Introduction et rappels
II) Présentation du modèle
III) Estimation (méthode des moindres carrés)
IV) Décomposition de la variance 
V) Test statistique et IC
VI) Exemple détaillé sous R

---

## I.  Introduction

Regression linéaire simple:

Considérons $x$ et $Y$ deux variables. Y est expliquée (modélisée) par  la variable explicative $x$. 

Modèle : $$E(Y)  = f(x) = \beta_0 + \beta_1 x$$

**la méthode des moindres carrés ordinaires (MCO)**: $$EQM(\beta_0, \beta_1) = \sum^{n}_{i=1} (Y_i - \widehat\beta_0 -  \widehat\beta_1x_i)^2$$

Soit $$ SC_{tot} = SC_{reg} + SC_{res}$$ 

**Objectif de la regression linéaire multiple**: chercher une relation entre la variable observée $Y$ et plusieurs variables explicative $X_1, ..., X_p$. 


---

## II. Présentation du modèle

Y est expliquée (modélisée) par  les variables explicatives $X_1, ..., X_p$. 

On applique alors le modèle linéaire suivant:

$$Y  = \beta_0 + \beta_1X_1 + ... + \beta_pX_p + \epsilon  $$

En pratique: 

Dans un échantillon de $n$ individus, nous mesurons $y_i, x_{i,1},...,x_{i,p}$ pour $i=1,...,n$.

Les variables $x_{i,j}$ sont fixes, tandis que les variables $Y_i$ sont aléatoires.

---

## II.  Présentation du modèle
### Exemple

```{r}
data(state)
stateEU <-  data.frame(state.x77, row.names=state.abb)
attach(stateEU)
```

Cette base de données comprend les mesures sur 50 pays des Etats-Unis de:

- Population: population estimée au 1er juillet 1975
- Income: revenu par individu (1974)
- Illiteracy: illettrisme (1970, pourcentage de la population)
- Life.Exp: espérance de vie moyenne (1969-1971)
- Murder:  taux d'homicide pour 100 000 individus (1976)
- HS Grad:  pourcentage de diplômés niveau baccalauréat --high-school graduates-- (1970)
- Frost: nombre de jours moyens avec des températures inférieures à 0$^o$C dans les grandes villes (1931-1960)
- Area : surface du pays en miles carrés.

---

## II.  Présentation du modèle
### Exemple

Représentation des donnée.


```{r}
# README RStudio config, uncheck: # preferences > R Markdown > show output inline for... 
layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
plot(Life.Exp, Murder, main="life.Exp~Murder")
plot(Life.Exp, HS.Grad, main="life.Exp~HS Grad")
```



---

## III.  Estimation - Méthode des moindres carrés
### Méthode


Pour l’instant, la droite de régression est inconnue.
Tout le problème est d’estimer les paramètres $\beta_0, ..., \beta_p$ du modèle de regression, à partir d’un échantillon de données.

Utiliser les **moindres carrés** revient à minimiser la quantité suivante: $$min_{\beta_0, ...,\beta_p}\sum^n_{i=1}\Big(y_i -(\beta_0+ \beta_1X_1 + ... + \beta_pX_p)\Big)^2$$

---

## III.  Estimation - Méthode des moindres carrés
### Version matricielle

Le système peut se réecrire

$$\left(\begin{array}
{rrr}
y_1\\
\vdots\\
y_n
\end{array}\right) = \left(\begin{array}
{rrr}
1 & x_{1,1} & ... & x_{1,p}\\
\vdots & \vdots & \vdots & \vdots \\
1 & x_{n,1} & ... & x_{n,p}
\end{array}\right)  \left(\begin{array}
{rrr}
\beta_0\\
\vdots\\
\beta_n
\end{array}\right) + \left(\begin{array}
{rrr}
\epsilon_1\\
\vdots\\
\epsilon_n
\end{array}\right)
$$

Avec $y = X\beta + \epsilon$ 

Et le vecteur des résidus $\widehat{e} = y - \widehat{y} = y - X\widehat\beta$


Les variables $y$ et $X$ sont mesurées tandis que l’estimateur $\beta$  est à déterminer.
La **méthode des moindres carrés ordinaires** consiste à trouver le vecteur $\beta$  qui minimise $||\epsilon||^2 =  {}^t{\epsilon}\epsilon$.  

---

## III.  Estimation - Méthode des moindres carrés
### Calcul

\begin{eqnarray}
         ||\epsilon||^2    & = & {}^t(y-X\widehat\beta) (y-X\widehat\beta)            \\
                           & = & {}^tyy - {}^t\widehat\beta{}^tXy - {}^tyX\widehat\beta + {}^t\widehat\beta{}^tXX\widehat\beta           \\
                           & = & {}^tyy - 2{}^t\widehat\beta^t(X)y - {}^t\widehat\beta{}^tXX\widehat\beta     
\end{eqnarray}

car ${}^t\widehat\beta{}^tXy$ est un scalaire. Donc il est égal à sa transposée.

La dérivée par rapport à $\beta$ est alors égale à : $−2{}^tXy+2{}^tXX\widehat\beta$.


---

## III.  Estimation - Méthode des moindres carrés
### Calcul

**Objectif** :  Nous cherchons $\beta$  qui annule cette dérivée. Donc nous devons résoudre l’équation suivante :

$$^tXX\widehat\beta = {}^tXy$$
Nous trouvons après avoir inversé la matrice ${}^tXX$ (il faut naturellement vérifier que ${}^tXX$ est carrée et inversible c’est-à-dire qu’aucune des colonnes qui compose cette matrice ne soit proportionnelle aux autres colonnes)
$$\widehat\beta  = ({}^t XX)^{-1}{}^{t} Xy.$$

---

## III.  Estimation - Méthode des moindres carrés
### Application à la régression linéaire simple (p=2), pour info

$$\left(\begin{array}
{rrr}
{}^tXX
\end{array}\right) = \left(\begin{array}
{rrr}
n & \sum{x_i}\\
\sum{x_i} & \sum{x_i}^2 
\end{array}\right) ;  \left(\begin{array}
{rrr}
{}^tXy
\end{array}\right) = \left(\begin{array}
{rrr}
\sum{y_i}\\
\sum{x_iy_i}
\end{array}\right)
$$

Donc

$$\left(\begin{array}
{rrr}
({}^tXX)^{-1}
\end{array}\right) = \frac{1}{n\sum{x_i^2 - (\sum{x_i^2})^2}}
\left(\begin{array}
{rrr}
\sum{x_i}^2 & -\sum{x_i}\\
-\sum{x_i} & n 
\end{array}\right) 
=  \frac{1}{\sum{x_i - \overline{x}_n}}
\left(\begin{array}
{rrr}
\sum{x_i}^2/n & -\overline{x}_n\\
-\overline{x}_n & 1 
\end{array}\right) 
$$


---

## IV.  Analyse de la variance
### Variation expliquée et inexpliquée

La variation de $Y$ vient du fait de sa dépendance aux variables explicatives $X$. C'est la **variation expliquée par le modèle**.


Variation totale de Y = Variation expliquée par le modèle + Variation inexpliquée par le modèle


La méthode des moindres carrés conserve la décomposition (en variance expliquée et résiduelle) en considérenat la somme des carrés de ces différences: 

$$ \sum(y_i - \overline{y}_n)^2 = \sum(\widehat{y}_i - \overline{y}_n)^2 + \sum(y_i - \widehat{y}_i)^2 $$
Soit $$ SC_{tot} = SC_{reg} + SC_{res}$$ 

Avec $SC_{tot}$ la somme des carrés totale,  $SC_{reg}$ la somme des carrés due à la régression et  $SC_{res}$ la somme des carrés des résidus.


---

## IV.  Analyse de la variance
### Coefficient de determination $R^2$

La mesure du pourcentage de la variation totale expliquée par le modèle se fait par le **coefficient de détermination**.

$$ R^2 = \frac{SC_{reg}}{SC_{tot}}$$

- $R^2$ est compris entre 0 et 1.
- $R^2 = 1$ : cas où les données sont parfaitement alignées
(comme c’est le cas pour un modèle déterministe).
- $R^2 = 0$ :cas où la variation de $Y$ n’est pas due à la variation de $X$. Les données ne sont pas du tout alignées.
- Plus $R^2$  est proche de 1, plus les données sont alignées sur la droite de régression.


---

## V. Test statistique et IC
### Hypothèses fondamentales

Il est important de noter que la construction du modèle de régression et l’estimation des paramètres par MCO ne fait pas appel aux hypothèses de distribution

Les hypothèses de distribution sont essentielles lorsqu’il s’agit de construire des tests et des intervalles de confiance et de prédiction.

Hypothèses fondamentales:

- Les observations sont indépendantes
- La variance des erreurs est constante $\sigma^2$
- La loi des erreurs est une loi normale $\mathcal{N}(0, \sigma^2)$

---

## V. Test statistique et IC
### Hypothèse fondamentales


$y = X\beta + \epsilon$ où le vecteur aléatoire $\epsilon$ suit une loi multinormale qui vérifie les hypothèses suivantes:
$$ \mathbb{E}[\epsilon] = 0 $$

$$ Var[\epsilon] = \sigma^2I_n $$
où $\sigma^2$ est la variance de la population et $I_n$ est la matrice identité de taille $n$.

Les hypothèses précédentes impliquent $\mathbb{E}[y] = X\beta$ et $Var[y] = \sigma^2I_n$.

---

## V. Test statistique et IC
### Hypothèse fondamentales

Sous ces hypothèses, on peut alors démontrer les propriétés des estimateurs: 

$\mathbb{E}[\widehat\beta] = \beta$ : estimateur sans biais

$Var[\widehat\beta] = \sigma^2({}^tXX)^{-1}$

La variance $\sigma^2$ est inconnue. Il faut l'estimer:

$$CM_{res} = \frac{\sum(y_i -\widehat{y}_i)^2}{n-p} = \frac{SC_{res}}{n-p} = \frac{SC_{tot}-SC_{reg}}{n-p}$$

où $n$ est le nombre d'individus/observations, $p$ est le nombre de variables explicatives, et $(n-p)$ le nombre de degrés de liberté associé à $SC_{res}$.

---

## V. Test statistique et IC
### Etudes des résidus

**Représentation graphique**

On représente les principaux graphiques sur les résidus.

```{r}
life.lm = lm(Life.Exp ~ Murder + HS.Grad + Frost, data = stateEU)
layout(matrix(1:4, 2, byrow=TRUE), respect=TRUE)
plot(life.lm)
```

Toutes les hypothèses semblent véri ées : pas de structure dans le premier graphique ni dans le troisième, pas d'homoscédasticité au vu du premier graphique, hypothèse gaussienne non remise en cause par le QQ-plot et aucun point aberrant d'après la distance de Cook.

On vérifie plus précisément l'hypothèse d'indépendance des résidus. On peut représenter l'autocorrélation des résidus

```{r}
acf(residuals(life.lm),ci=0.99)
```


L'hypothèse d'indépendance est cohérente graphiquement, au seuil 0.99. Nous avons choisi ici d'être peu restrictifs sur l'autocorrélation en prenant un intervalle de con ance de niveau 0.99, mais elle ne serait pas valide avec une con ance de 0.95.

**Avec des tests statistiques**

```{r}
shapiro.test(resid(life.lm))
```
On obtient une p-valeur de 10.4%. On ne rejette pas l'hypothèse de normalité avec un seuil de 5%.
On teste l'hypothèse de linéarité :

```{r}
library(lmtest)
raintest(life.lm)
```
On obtient une p-valeur de 32.98%. On ne rejette pas le modèle linéaire. On teste ensuite l'hypothèse d'homogénéité des variances :

```{r}
gqtest(life.lm)
```
On obtient une p-valeur de 75.62%. L'homogénéité de la variance des résidus n'est pas rejetée.
On teste également l'indépendance des résidus :

```{r}
dwtest(life.lm)
```

On obtient une p-valeur de 23.23%. L'indépendance des résidus n'est pas rejetée (Attention, le test de Durbin Wastson teste l'autocorrélation d'ordre 1 et d'autres tests peuvent être plus adaptés   test de Box-Pierce par exemple).
Nous vérifions enfin qu'il n'y a pas de colinéarité forte dans le modèle, c'est-à-dire que les variables explicatives ne sont pas linéairement dépendantes.

```{r}
library(car)
vif(life.lm)
```

Les valeurs des VIF (Variance Inflation Factors) étant inférieures à 10, les variables ne présentent pas de forte colinéarité.

---

## V. Test statistique et IC
### Analyse de la variance: Test de Fisher

On se pose la question de l’effet des variables $X$ :

$\mathcal{H}_0 : {\beta_1 = ... = \beta_p = 0}$ contre $\mathcal{H}_1 : \exists$ au moins un $j$ tel que ${\beta_j \neq 0}$ où $j$ varie de $1$ à $p$.

*Remarque*: Si l'hypothèse nulle est vérifiée, alors $y_i = \beta_0 + \epsilon_i$

---

## V. Test statistique et IC
### Analyse de la variance: Test de Fisher

Tableau de l'analyse de la variance

| Source de variation   | sc        | ddl  |  cm  | $F_{obs}$
| :-------------------- | :-------: | :--: | :--: | ---:
| Régression            | $sc_{reg} = \sum^n_{i=1}(\widehat{y}_i - \overline{y})^2$    | p-1  | $\frac{sc_{reg}}{p-1}$ | $\frac{cm_{reg}}{cm_{res}}$
| Résiduelle            | $sc_{res} = \sum^n_{i=1}(y_i - \widehat{y})^2$    | n-p  | $\frac{sc_{reg}}{n-p}$ 
| Totale            | $sc_{tot} = \sum^n_{i=1}(y_i - \overline{y})^2$    | n-1  

---

## V. Test statistique et IC
### Test de Fisher : Méthode

1) Calculer la statistique
$$F_{obs} = \frac{CM_{reg}} {CM_{res}}$$

2) Lire la valeur critique $F_{1−\alpha,p−1,n−p}$ où $F_{1−\alpha,p−1,n−p}$ est le $(1 − \alpha)$-quantile d’une loi de Fisher avec $(p − 1)$ et $(n − p)$ degrés de liberté, car si l’hypothèse nulle $\mathcal{H}_0$ est vraie, alors $F_{obs}$ suit une loi de Fisher avec $(p − 1)$ et $(n − p)$ degrés de liberté.

3) Comparer la statistique c’est-à-dire la valeur observée à la valeur critique.

---

## V. Test statistique et IC
### Test de Fisher : Règle de décision

Nous décidons de rejeter l’hypothèse nulle  $\mathcal{H}_0$ et d’accepter l’hypothèse alternative  $\mathcal{H}_1$, au seuil $\alpha = 5\%$, si
$$|F_{obs}| \geq F_{1−\alpha,p−1,n−p}$$

Nous décidons de ne pas rejeter l’hypothèse nulle $\mathcal{H}_0$ et
donc de l’accepter si
$$|F_{obs}| < F_{1−\alpha,p−1,n−p}$$

---

## V. Test statistique et IC
### Test de Fisher: exemple avec R


```{r}
summary(life.lm)
```
Pour lire la p valeur du test de Fisher et celles des tests de Student, il faut s’assurer auparavant que les résidus suivent une loi normale. Pour cela, vous allez réaliser un test de normalité, celui de Shapiro-Wilk avec R.


La p-valeur (p-value = .634e-12 ) du test de Fisher étant inférieure ou égale à  $\alpha = 5\%$, le test est significatif. Nous rejetons H0 et nous décidons que H1 est vraie avec un risque de première espèce  $\alpha = 5\%$. Donc il y a au moins une des deux variables qui joue le rôle de variable explicative.


---

## V. Test statistique et IC/IP
### Analyse de la variance: Test de Student

On se pose la question de l’effet de la variable $x$ :

$\mathcal{H}_0 : {\beta_j = 0}$ contre $\mathcal{H}_1 : {\beta_j \neq 0}$ pour $j = 0, ..., p$

**Conditions d’applications du test** : Les variables aléatoire $\epsilon_i$ sont indépendantes et suivent la loi normale centrée et de variance $\sigma^2$.

*Test d’ajustement des résidus à une gaussienne* : Test de shapiro, Kolmogoroff-Smirnoff, QQ plot

**Statistique du test** : Si l’hypothese nulle $\mathcal{H}_0$ est vérifiée, alors la variable aléatoire $\mathcal{T}_{\widehat\beta_j,n-2} = \frac{\widehat\beta_j-0}{s_{\widehat\beta_j}}$ suit la loi de Student $\mathcal{T}(n-2)$

**Décision et conclusion du test**: La valeur critique du test, notée $c_\alpha$ est lue dans une table de la loi de Student.

- Si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $\mathcal{T}_{\widehat\beta_1,n-2}(obs)$ est supérieure ou égale à $c_\alpha$, alors le test est significatif. Vous rejetez  $\mathcal{H}_0$ et vous décidez que $\mathcal{H}_1$ est vraie avec un risque d’erreur de première espèce $\alpha$.

- Si la valeur absolue de la valeur de la statistique calculée sur l’echantillon, notée $\mathcal{T}_{\widehat\beta_1,n-2}(obs)$ est strictement inférieure à $c_\alpha$, alors le test n’est pas significatif. Vous conservez $\mathcal{H}_0$ avec un risque d’erreur de deuxième espèce $\beta$ à évaluer. 

---

## V. Test statistique et IC
### Test de Student: exemple avec R


```{r}
summary(life.lm)
```

La p-valeur (p-value = 8.04e-10) du test de Student, associée à
« Murder » étant inférieure ou égale à  $\alpha = 5\%$, le test est significatif. Nous rejetons H0 et nous décidons que H1 est vraie avec un risque de première espèce  $\alpha = 5\%$.

La p-valeur (p-value = 0.00195  ) du test de Student, associée à
« HS.Grad » étant inférieure ou égale à  $\alpha = 5\%$, le test est significatif. Nous rejetons H0 et nous décidons que H1 est vraie avec un risque de première espèce  $\alpha = 5\%$.

La p-valeur (p-value = 0.00699  ) du test de Student, associée à
« Frost » étant inférieure ou égale à  $\alpha = 5\%$, le test est significatif. Nous rejetons H0 et nous décidons que H1 est vraie avec un risque de première espèce  $\alpha = 5\%$.

---

## V. Test et IC
### Intervalle de confiance

On peut construire les intervalles de confiance suivants

$$IC_{1-\alpha}(\widehat\beta_j) = \Big]\widehat\beta_j - t_{n-2;1-\alpha/2}*s_{\beta_j}; \widehat\beta_j + t_{n-2;1-\alpha/2}*s_{\beta_j}\Big[$$

Cet intervalle de confiance est construit de telle sorte qu’il contienne le paramètre inconnu $\beta_j$ avec une probabilité de $(1−\alpha)$.
 
---

## V. Test et IC
### IC/IP exemples sous R

1) Intervalle de confiance
```{r}
confint(life.lm)
ci=  c(life.lm$coefficients - 1.96*summary(life.lm)$coefficients[,2], life.lm$coefficients+ 1.96*summary(life.lm)$coefficients[,2])
ci
```

2) Intervalle de prédiction

Supposons que nous disposions des données pour un nouveau pays, par exemple européen. Nous souhaitons comparer l'espérance de vie estimée avec ce modèle avec l'espérance de vie observée. L'intervalle de con ance sur la valeur prédite est donné par l'instruction suivante :

```{r}
life.pred = predict(life.lm,data.frame(Murder=8,  HS.Grad=75, Frost=80, Population=4250), interval="confidence",se.fit = TRUE)
ci=  c(life.pred$fit[1] - 1.96*life.pred$se.fit, life.pred$fit[1]+ 1.96*life.pred$se.fit)
life.pred
ci
```

---

## VI. Sélection de variables

- Critères de choix

  -- Critère du $R^2$ 
  
  -- Critètre d’information d’Akaike (AIC)
  
  -- Critètre d’information bayesien (BIC)

- Procédure de sélection de variable

-- Forward (ou pas à pas ascendante)

-- Backward (ou pas à pas descendante)

-- Stepwise

---

## VI. Sélection de variables
### Les critères

### Pseudo-$R^2$ (Cox-Snell, Nagelkerke)

Augmente de façon monotone avec l’introduction de nouvelles variables 


### Critètre d'information d'Akaike (AIC)

Le critère AIC est défini par : 

$$AIC = n*log(\frac{SC_{res}}{n}) + 2(p+1)$$

### Critètre d'information bayesien (BIC)

Le critère BIC est défini par : 

$$BIC =  n*log(\frac{SC_{res}}{n}) + log(n)(p+1)$$

Pour l’AIC et le BIC : 

- le critère s’applique aux modèles estimés par une méthode du **maximum de vraisemblance**
- le meilleur modèle est celui qui minimise le critère


Le BIC est plus parcimonieux que l’AIC puisqu’il pénalise plus le nombre de variables présentent de le modèle.

---

## VI. Sélection de variables
### Les méthodes

Méthodes les plus classiques:

- Forward (ou pas à pas ascendante)

- Backward (ou pas à pas descendante)

- Stepwise

Ces méthodes s’appuient sur les **données recueillies**

Elles sont **itératives**

Elle dépendent de **paramètres** (à valeur prédéfinie)

Bien que l’efficacité de ces méthodes ne puisse être démentie par la pratique, il ne serait pas raisonnable de se fier uniquement aux résultats statistiques fournis par un
algorithme. En effet, pour décider d’ajouter ou de supprimer une variable dans un modèle, il faut conserver :

- une part d’intuition

- une part de déduction

- une part de synthèse

---

## VI. Sélection de variables
### Exemple détaillé sous R


On souhaite expliquer l'espérance de vie Life.Exp en fonction des autres variables. On va utiliser pour cela une méthode descendante.

```{r}
extractAIC(life.lm)
```

Si nous souhaitons minimiser le critère AIC, nous pouvons l'obtenir de la façon suivante :

```{r}
life.lm <- lm(Life.Exp ~ ., data=stateEU) 
summary(life.lm)
```

(On préférera extractAIC à la fonction AIC qui donne un critère légèrement di érent.) On commence par enlever les variables dont la p-valeur est supérieure à 0.3.

```{r}
life.lm <-  update(life.lm,.~.-Area-Illiteracy-Income)
summary(life.lm)
```
On constate que l'AIC a bien diminué.
On enlève ensuite la variable dont le coe cient est le moins signi catif, ici Population.

```{r}
life.lm <-  update(life.lm,.~.-Population)
summary(life.lm)
```

On constate que l'AIC a augmenté (et que l'écart-type estimé des résidus a augmenté : il vaut 0.7427 contre 0.7197 auparavant). On préfère donc le modèle conservant la variable Population.

```{r}
life.lm <- lm(Life.Exp ~ Murder + HS.Grad + Frost + Population)
```
R peut faire ce raisonnement de manière automatisée. Il suffit d'appliquer 

```{r}
life.lm <- step(lm(Life.Exp ~ .,data=stateEU), method="bakward")
```

Le critère choisi par défaut est alors l'AIC. Il est également possible de choisir les méthodes "forward" et "both". Ensuite on peut résumer les di érentes étapes de la façon suivante :

```{r}
life.lm$anova
```
On peut également utiliser les fonctions add1 et drop1, non détaillées ici.


---

## Notes

Ce cours s’inspire des références suivantes:

- Frédéric Bertrand & Myriam Maumy-Bertrand
- Franck Picard
- Irenne Ganaz